= 10장 k-평균 군집화 : 항목표시가 없는 아이템 그룹 짓기
* 군집화는 자동으로 유사한 것끼리 묶어주는 비지도 학습의 한가지 유형
* k-means는 k개의 군집이 존재하며, 각 군집의 중심은 평균값라는 의미이다.
* 때로는 군집화를 비지도 분류(unsupervised classification) 이라 하지만, 미리정해놓은 분류 항목이 없다는 것이 일반 분류와의 차이점이다.
* 유사도 측정 방법은 응용 프로그램에 따라 다르다.
* 학습 순서
** 간단한 k-means 클러스터링 알고리즘 몇가지 소개
** 몇가지 문제점을 해결하며 더 좋은 군집을 생산하기 위한 후처리 적용
** 더 효과적인 양분하는 k-means(bisecting k-means) 적용
** bisecting k-means 를 이용해 최적의 주차장소 찾는 예제 진행

== 10.1 k-평균 군집화 알고리즘
* 특징
** 장점 : 구현이 쉽다
** 단점 : 지역 최소점에 수렴될 수 있으며, 매우 큰 데이터 집합의 경우 처리 시간이 오래걸린다.
** 활용 : 수치형

[source,python]
----
from numpy import *

def loadDataSet(fileName):      #general function to parse tab -delimited floats
    dataMat = []                #assume last column is target value
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = map(float,curLine) #map all elements to float()
        dataMat.append(fltLine)
    return dataMat

def distEclud(vecA, vecB):
    return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)

# 각 피쳐별로 최대/최소값 내에서 임의의 값을 선택해 중심점으로 잡음
def randCent(dataSet, k):
    n = shape(dataSet)[1]
    centroids = mat(zeros((k,n)))#create centroid mat
    for j in range(n):#create random cluster centers, within bounds of each dimension
        minJ = min(dataSet[:,j])
        rangeJ = float(max(dataSet[:,j]) - minJ)
        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1))
    return centroids

datMat = mat(loadDataSet('testSet.txt'))
print min(datMat[:, 0]) # [[-5.379713]]
print min(datMat[:, 1]) # [[-4.232586]]
print max(datMat[:, 0]) # [[4.838138]]
print max(datMat[:, 1]) #[[5.1904]]
print datMat[0] # [[1.658985 4.285136]]
print datMat[1] # [[-3.453687  3.424321]]
print distEclud(datMat[0], datMat[1])   # 5.184632816681332
----

[source,python]
----
def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m,2)))#create mat to assign data points
                                      #to a centroid, also holds SE of each point
    centroids = createCent(dataSet, k)
    clusterChanged = True
    while clusterChanged:
        clusterChanged = False
        for i in range(m):  # for each data point assign it to the closest centroid
            minDist = inf; minIndex = -1
            for j in range(k):
                distJI = distMeas(centroids[j,:],dataSet[i,:])
                if distJI < minDist:
                    minDist = distJI; minIndex = j
            if clusterAssment[i, 0] != minIndex: clusterChanged = True
            clusterAssment[i, :] = minIndex, minDist**2
        #print centroids
        for cent in range(k):   # recalculate centroids
            ptsInClust = dataSet[nonzero(clusterAssment[:, 0].A == cent)[0]]   # get all the point in this cluster
            centroids[cent, :] = mean(ptsInClust, axis=0)    # assign centroid to mean
    return centroids, clusterAssment

datMat = mat(loadDataSet('testSet.txt'))
myCentroids, clustAssign = kMeans(datMat, 4)
#[[ 2.65077367 -2.79019029], [-2.46154315  2.78737555], [ 2.6265299   3.10868015], [-3.53973889 -2.89384326]]
----


== 10.2 후처리로 군집 성능 개선하기
* 간혈적으로 k-means를 수행한 결과가 전체 최소점이 아닌 지역 최소점(local-minimum)에 수렴해 엉뚱한 결과를 야기하곤 한다.
* 이를 해소하기 위해 10.3에서 살펴볼 양분하는 k-means (bisection k-means)를 이용한다.

== 10.3 Bisection k-means

[source,python]
----
def biKmeans(dataSet, k, distMeas=distEclud):
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m, 2)))
    centroid0 = mean(dataSet, axis=0).tolist()[0]
    centList = [centroid0]  # create a list with one centroid
    for j in range(m):  # calc initial Error
        clusterAssment[j, 1] = distMeas(mat(centroid0), dataSet[j, :]) ** 2
    while len(centList) < k:
        lowestSSE = inf
        for i in range(len(centList)):
            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, 0].A == i)[0], :]  # get the data points currently in cluster i
            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)
            sseSplit = sum(splitClustAss[:, 1])  # compare the SSE to the currrent minimum
            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0], 1])
            print "sseSplit, and notSplit: ",sseSplit,sseNotSplit
            if (sseSplit + sseNotSplit) < lowestSSE:
                bestCentToSplit = i
                bestNewCents = centroidMat
                bestClustAss = splitClustAss.copy()
                lowestSSE = sseSplit + sseNotSplit
        bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList)  # change 1 to 3,4, or whatever
        bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit
        print 'the bestCentToSplit is: ', bestCentToSplit
        print 'the len of bestClustAss is: ', len(bestClustAss)
        centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0]  # replace a centroid with two best centroids
        centList.append(bestNewCents[1, :].tolist()[0])
        clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss # reassign new clusters, and SSE
    return mat(centList), clusterAssment
----
