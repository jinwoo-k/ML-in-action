= 4장. 나이브 베이즈
확률 이론으로 분류하기

== 4.1 베이지안 의사결정 이론으로 분류하기
* 장점
** 소량의 데이터를 가지고 작업이 이뤄지며, 여러개의 분류 항목을 다룰 수 있다.
* 단점
** 입력 데이터를 어떻게 준비하냐에 따라 민감하게 작용한다.
* 적용
** 명목형 값
* 나이브 베이즈를 이해하기 위해서는 조건부 확률을 이해해야함.

== 4.2 조건부 확률
* 조건부 확률 : p(x|c) = p(x and c) / p(c)
* 베이즈 규칙
** p(x|c), p(c), p(x) 를 알고 있는 상황에서 p(c|x)를 구하고자 하는 경우
** p(c|x) = p(x|c)p(c) / p(x)

== 4.3 조건부 확률로 분류하기
* p(ci|x,y) : x,y 라는 점이 주어졌을때 ci에 속할 확률
* 베이즈 규칙 적용 : p(ci|x,y) = p(x,y|ci)p(ci) / p(x,y)

== 4.4 나이브 베이즈로 문서 분류하기
* 나이브 베이즈에 대한 일반적인 접근 방법
. 수집 : 많은 방법이 있으며, 이번장에서는 RSS 자료 사용
. 준비 : 명목형 또는 부울 형 값이 요구된다.
. 분석 : 많은 속성들을 플롯하는건 도움이 되지 않음. 히스토그램이 가장 좋음
. 훈련 : 각 속성을 독립적으로 조건부 확률을 계산한다.
. 검사 : 오류율을 계산한다.
. 사용 : 나이브 베이즈의 일반적인 응용프로그램 중 하나는 문서분류이다. 어떤 분류를 설정하기 위해 나이브 베이즈를 사용할 수 있으며, 꼭 텍스트일 필요는 없다.

* 원래는 1000개의 속성이 있고, N개의 표본이 필요하다면 N^1000 개의 표본이 필요하게 된다.
* 하지만 모든 속성이 서로 독립이라는 가정을 하면 N*1000 개의 표본만 필요하게 된다.
* 나이브 베이즈의 나이브는 이렇게 모든 속성이 서로 독립이라는 가정을 하기 때문이다.

== 4.5 파이썬으로 텍스트 분류하기
* 온라인 전자 게시판을 위한 필터를 만든다. 작성자가 작성한 문서가 폭력적이라면 1을, 그렇지 않으면 0이라는 범주 값을 갖게 한다.

=== 4.5.1 준비 : 텍스트 단어 벡터 만들기
[source,python]
----
def loadDataSet():
    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not
    return postingList, classVec


def createVocabList(dataSet):
    vocabSet = set([])  # create empty set
    for document in dataSet:
        vocabSet = vocabSet | set(document)  # union of the two sets
    return list(vocabSet)


def setOfWords2Vec(vocabList, inputSet):
    returnVec = [0] * len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print "the word: %s is not in my Vocabulary!" % word
    return returnVec

listOPosts, listClasses = loadDataSet()
vocabList = createVocabList(listOPosts)
print vocabList
wordsVector = setOfWords2Vec(vocabList, listOPosts[0])
print wordsVector
----

=== 4.5.2 훈련 : 단어 벡터로 확률 계산하기
* p(ci|w) = p(w|ci)p(ci)/p(w)
** p(w)는 해당 문서가 나올 확률 (1/전체문서수)
** p(ci)는 해당 분류가 나올 확률
** p(w|ci)는 각각 속성들에 대한 표현으로 변경 p(w0, w1, w2...wN|ci)
** 모든 속성이 독립이라는 가정 하에 다음과 같이 표현 가능 : p(w0|ci)p(w1|ci)...p(wN|ci)
** p(w0|ci) 는 ci 분류의 문서에서 w0 단어가 나올 확률 (부정적 문서의 단어가 100개이고, w0 단어가 10번 등장했다면 1/10)

[source,python]
----
def trainNB0(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    p0Num = zeros(numWords); p1Num = zeros(numWords)
    p0Denom = 0.0; p1Denom = 0.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = p1Num / p1Denom          #change to log()
    p0Vect = p0Num / p0Denom          #change to log()
    return p0Vect,p1Vect,pAbusive
----

=== 4.5.3 검사 : 실제 조건을 반영하기 위해 분류기 수정하기
* 특정 단어가 발생할 확률이 0인 경우 p(w0|1)p(w1|1)... 의 식으로 인해 결과가 0이 됨
* 이를 방지하기 위해 단어수를 1로, 분모를 2로 초기화함
* 작은수를 곱하게되면 underflow 현상 발생 > log 를 이용해 이를 해결

[source,python]
----
def trainNB0(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    p0Num = ones(numWords); p1Num = ones(numWords)
    p0Denom = 2.0; p1Denom = 2.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = log(p1Num / p1Denom)          #change to log()
    p0Vect = log(p0Num / p0Denom)          #change to log()
    return p0Vect,p1Vect,pAbusive

def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):
    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult
    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)
    if p1 > p0:
        return 1
    else:
        return 0

def testingNB():
    listOPosts,listClasses = loadDataSet()
    myVocabList = createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))
    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))
    testEntry = ['love', 'my', 'dalmation']
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
    testEntry = ['stupid', 'garbage']
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)

testingNB()
----

=== 4.5.4 준비 : 중복 단어 문서 모델
* setOfWords2Vec() 메서드는 한개 문서내에 특정 단어가 한번이상 등장하더라도 모두 동일한 1로 세팅된다. 이를 변경하자

[source,python]
----
def bagOfWords2VecMN(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] += 1
    return returnVec
----

== 4.6 예제 : 스팸 이메일 분류하기
. 수집 : 제공된 텍스트 파일
. 준비 : 토큰벡터로 텍스트 구문 분석
. 분석 : 구문 분석이 정확하게 되었는지 토큰 검토
. 훈련 : 이전에 생성한 trainNB0() 메서드 이용
. 검사 : classifyNB() 를 사용하고 문서 집합에서 오류율을 계싼하는 새로운 검사 함수 생성
. 사용 : 완전한 프로그램을 구축하여 문서들을 분류하고 화면에 잘못 분류된 문서 출력

=== 4.6.1 준비 : 텍스트 토큰 만들기
* 문자열을 단어가 아닌 문자로 분할한 후 길이가 0인 단어 제거

[source,python]
----
import re
regEx = re.compile("\\W*")
mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'
listOfTokens = regEx.split(mySent)
toks = [tok for tok in listOfTokens if len(tok) > 0]
print toks
----

=== 4.6.2 검사 : 나이브 베이즈로 교차 검증하기
[source,python]
----
def textParse(bigString):    #input is big string, #output is word list
    import re
    listOfTokens = re.split(r'\W*', bigString)
    return [tok.lower() for tok in listOfTokens if len(tok) > 2]

def spamTest():
    docList=[]; classList = []; fullText =[]
    for i in range(1,26):
        wordList = textParse(open('email/spam/%d.txt' % i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1)
        wordList = textParse(open('email/ham/%d.txt' % i).read())
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    vocabList = createVocabList(docList)#create vocabulary
    trainingSet = range(50); testSet=[]           #create test set
    # extract 10 testSets
    for i in range(10):
        randIndex = int(random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])
    trainMat=[]; trainClasses = []
    for docIndex in trainingSet:#train the classifier (get probs) trainNB0
        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))
        trainClasses.append(classList[docIndex])
    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))
    errorCount = 0
    for docIndex in testSet:        #classify the remaining items
        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])
        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:
            errorCount += 1
            print "classification error",docList[docIndex]
    print 'the error rate is: ',float(errorCount)/len(testSet)
    #return vocabList,fullText
----

== 4.7 예제 : 나이브 베이즈를 사용하여 개인 광고에 포함된 지역 특색 도출하기
* 지역별로 다른 단어를 사용한다는 가정, 지역별로 사용되는 단어를 나이브 베이즈를 이용해 찾는다.
. 수집 : RSS 피드로부터 수집, RSS 피드의 인터페이스 구축
. 준비 : 토큰 벡터로 텍스트 구문 분석
. 분석 : 구문 분석이 확실하게 되었는지 토큰 검사
. 훈련 : 이전에 생성한 trainNB0() 사용
. 검사 : 실질적으로 동작하는지 확인하기 위해 오류율 확인, 오류율과 결과를 개선하기 위해 토큰화 수정
. 사용 : 모든 상황을 함께 다루는 완전한 프로그램 구축. 두가지 RSS 피드에서 얻은 가장 일반적인 단어 표현

=== 4.7.1 수집 : RSS 피드 불러오기

[source,python]
----
def calcMostFreq(vocabList,fullText):
    import operator
    freqDict = {}
    for token in vocabList:
        freqDict[token]=fullText.count(token)
    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True)
    return sortedFreq[:30]

def localWords(feed1,feed0):
    import feedparser
    docList=[]; classList = []; fullText =[]
    minLen = min(len(feed1['entries']),len(feed0['entries']))
    for i in range(minLen):
        wordList = textParse(feed1['entries'][i]['summary'])
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(1) #NY is class 1
        wordList = textParse(feed0['entries'][i]['summary'])
        docList.append(wordList)
        fullText.extend(wordList)
        classList.append(0)
    vocabList = createVocabList(docList)#create vocabulary
    print 'length of vocabList is ', len(vocabList)
    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words
    for pairW in top30Words:
        if pairW[0] in vocabList: vocabList.remove(pairW[0])
    trainingSet = range(2*minLen); testSet=[]           #create test set
    for i in range(5):
        randIndex = int(random.uniform(0,len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])
    trainMat=[]; trainClasses = []
    for docIndex in trainingSet:#train the classifier (get probs) trainNB0
        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))
        trainClasses.append(classList[docIndex])
    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))
    errorCount = 0
    for docIndex in testSet:        #classify the remaining items
        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])
        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:
            errorCount += 1
    print 'the error rate is: ',float(errorCount)/len(testSet)
    return vocabList,p0V,p1V

import feedparser
ny = feedparser.parse('https://newyork.craigslist.org/search/sss?format=rss')
sf = feedparser.parse('https://sfbay.craigslist.org/search/sss?format=rss')
print len(ny['entries'])
print len(sf['entries'])
print ny['entries'][0]

localWords(ny, sf)
localWords(ny, sf)
localWords(ny, sf)
localWords(ny, sf)
----

=== 4.7.2 분석 : 지역적으로 사용되는 단어 표현하기
[source,python]
----
def getTopWords(ny,sf):
    import operator
    vocabList,p0V,p1V=localWords(ny,sf)
    topNY=[]; topSF=[]
    for i in range(len(p0V)):
        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))
        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))
    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)
    print "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**"
    for item in sortedSF:
        print item[0]
    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)
    print "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**"
    for item in sortedNY:
        print item[0]
----
