= 2장. k-최근접 이웃 알고리즘
이해하기 쉽고 효과적인 알고리즘

== 1. 거리 측정을 이용하여 분류하기
* 장점
  ** 높은 정확도
  ** 오류 데이터(outlier)에 둔감
  ** 데이터에 대한 가정이 없음
* 단점
  ** 계산 비용이 높음
  ** 많은 메모리 요구
* 적용
  ** 수치형 값
  ** 명목형 값
* 알고리즘 개요
  ** 분류항목표시(label)이 붙어있는 학습데이터와 분류항목표시가 없는 새로운 데이터가 존재할때 새로운 데이터의 분류항목표시를 예측
  ** 새로운 데이터와 가장 가까운 거리를 갖는 N개의 학습데이터를 찾은 후 다수결에 따라 분류항목표시 결정
  ** 거리는 유클리드 거리를 이용
* 알고리즘을 kNN.py 파일에 작성하고 해당 모듈을 로드(import kNN)
[source,python]
----
from numpy import *
import operator

# 테스트로 사용할 데이터 셋 구성
def createDataSet():
    group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]])
    labels = ['A', 'A', 'B', 'B']
    return group, labels

# 분류 메서드
def classify0(inX, dataSet, labels, k):
    dataSetSize = dataSet.shape[0]

    # tile: 파라미터로 받은 행렬을 행/열에 대해 n, k 회 만큼 반복하는 신규 행렬 반환
    # tile(A: array_like, reps: array_like)
    # A : the input array
    # reps : The number of repetitions of A along each axis.
    todo tile 메서드 확인 (특정 열을 반복적으로 사용하는 매트릭스를 만들꺼라 추정, 모든 열에 대해 거리를 계산하기 위함)
    # diffMat 은 inX를 dataSet 열만큼 반복해서 만든 후 dataSet과 차를 구한다.
    diffMat = tile(inX, (dataSetSize, 1)) - dataSet 
    sqDiffMat = diffMat ** 2
    # 각 열의 모든 컬럼값을 더한다 > 결과는 열백터가 됨
    sqDistances = sqDiffMat.sum(axis = 1) 
    # 해당 작업까지가 각 열별 유클리드를 구하는 부분이다. 
    distances = sqDistances ** 0.5 

	# 거리별로 정렬된 인덱스 반환
    sortedDistIndicies = distances.argsort()	
    # 레이블별 발생빈도를 저장하기 위한 딕셔너리
    classCount = {}

    # 가장 가까운 k 개의 레이블을 추출해서 딕셔너리에 저장 또는 갱신
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1

	# 딕셔너리에 저장된 데이터를 값 내림차순으로 정렬
    sortedClassCount = sorted(classCount.iteritems(),
		key = operator.itemgetter(1), reverse = True)
	# 정렬후 첫번째 데이터 (가장 많은 값을 갖는 값)을 결과로 반환
    return sortedClassCount[0][0] 
----
* 오류율 : 잘못분류횟수(오류횟수) / 데이터검사횟수
