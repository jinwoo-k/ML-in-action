= 8장. 회귀: 수치형 값 예측하기
기존까지는 명목형 값을 예측하는 분류에 대해 배웠으며, 이번장에서는 연속적인 갓을 예측하는 회귀를 배운다.

== 8.1 회귀로 최적선 찾기
* 선형회귀의 장단점
** 장점 : 결과를 해석하기 쉽고 계산 비용이 적다.
** 단점 : 비선형 데이터를 모델링하기에 적합하지 않다.
** 활용 : 수치형, 명목형
* 선형회귀란
** 회귀는 선형, 비선형이 있으며, 일반적으로 선형회귀를 나타냄, 이번장은 선형회귀만 다룸
** 비선형회귀 예 : 마력 = 0.01 * 연봉 / 라디오를 듣는 시간
* 회귀의 접근방법
. 수집 : 모든방법
. 준비 : 회귀를 위해 수치형 값 필요, 명목형 값은 이진형 값으로 변환 필요
. 분석 : 2D 플롯을 통해 시각화, 축소방법을 적용하면 회귀 가중치 역시 시각화 가능
. 훈련 : 회귀 가중치를 찾는다.
. 검사 : 모델의 성과를 측정하기 위해 R2 혹은 예측 값과 데이터간의 상관관계 측정
. 사용 : 회귀를 가지고 입력 수치에 대한 수치형 값 예측. 이산적인 범주형 값이 아닌 연속형 값 예증
* 최소 자승법(최소제곱법, OLS : Ordinary Least Squares)
** 데이터 X, y로부터 w 를 구하는 방법
** 예측 값을 행렬표현식으로 표현 : X~1~^T^w=y~1~
** 데이터들에 대해 오차를 가장 적게하는 w 구하기
** 오차 제곱합(제곱오류) : image:imgs/8_1_3.png[,150]
** 위의 오차제곱을 행렬표기법으로 표기 : (y - Xw)^T^ (y - Xw)
** 위의 식을 w에 대해 미분 : X^T^ (y - Xw)
** 위의 식이 0이 되도록 풀면 w` = (X^T^ X)^-1^ X^T^ y
** 위의 식에서 `(사실은 ^, hat)은 현재 데이터에 대한 가장 좋은 추정치를 표현
** 위의 식에서는 다음과 같은 역행렬이 존재해야 처리 가능하다. (X^T^ X)^-1^

image::imgs/8_1_4.png[,800]

[source,python]
----
from numpy import *

def loadDataSet(fileName):      #general function to parse tab -delimited floats
    numFeat = len(open(fileName).readline().split('\t')) - 1 #get number of fields
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr =[]
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat,labelMat

def standRegres(xArr,yArr):
    xMat = mat(xArr); yMat = mat(yArr).T
    xTx = xMat.T*xMat
    if linalg.det(xTx) == 0.0:
        print "This matrix is singular, cannot do inverse"
        return
    ws = xTx.I * (xMat.T*yMat)
    return ws

    def plot(xMat, yMat, ws):
        import matplotlib
        matplotlib.use('TkAgg')
        import matplotlib.pyplot as plt
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.scatter(xMat[:, 1].flatten().A[0], yMat.T[:, 0].flatten().A[0])
        xCopy = xMat.copy()
        xCopy.sort(0)
        yHat = xCopy * ws
        ax.plot(xCopy[:, 1], yHat)
        plt.show()

    xArr, yArr = loadDataSet('ex0.txt')
    ws = standRegres(xArr, yArr)
    xMat = mat(xArr)
    yMat = mat(yArr)
    plot(xMat, yMat, ws)
----

* 예측된 값과 실제값과의 상관계수를 통해 예상값이 얼마나 정확한지 확인 가능

[source,python]
----
yHat = xMat * ws
print corrcoef(yHat.T, yMat)
----

== 8.2 지역적 가중치가 부여된 선형 회귀
* 선형회귀는 일반적으로 unerfit 되는 경향을 보임
* LWLR (locally weighted linear regression)
** 이번장에서는 추정값에 편향(bias)를 추가해 MSE 를 줄이는 방법인 LWLR 을 다룬다.
** KNN 처럼 예측시마다 새로운 연산을 필요로 한다.
** w' = (X^T^ W X)^-1^ X^T^ Wy
** 여기서 W는 가중치 대각행렬
** 가중치 할당식 : image:imgs/8_2_1.png[,150]
** 점 x 와 가까운 점일 수록 높은 가중치 부여
** k 값에 따라 다음과 같은 가중치 변화를 보임
** image:imgs/8_2_2.png[,400]

[source,python]
----
def lwlr(testPoint,xArr,yArr,k=1.0):
    xMat = mat(xArr); yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye((m)))
    for j in range(m):                      #next 2 lines create weights matrix
        diffMat = testPoint - xMat[j,:]     #
        weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))
    xTx = xMat.T * (weights * xMat)
    if linalg.det(xTx) == 0.0:
        print "This matrix is singular, cannot do inverse"
        return
    ws = xTx.I * (xMat.T * (weights * yMat))
    return testPoint * ws

def lwlrTest(testArr,xArr,yArr,k=1.0):  #loops over all the data points and applies lwlr to each one
    m = shape(testArr)[0]
    yHat = zeros(m)
    for i in range(m):
        yHat[i] = lwlr(testArr[i],xArr,yArr,k)
    return yHat

xArr, yArr = loadDataSet('ex0.txt')
yHat = lwlrTest(xArr, xArr, yArr, 0.001)

xMat = mat(xArr)
srtInd = xMat[:, 1].argsort(0)
xSort = xMat[srtInd][:, 0, :]

import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(xSort[:, 1], yHat[srtInd])
ax.scatter(xMat[:, 1].flatten().A[0], mat(yArr).T[:, 0].flatten().A[0], s=2, c='red')
plt.show()
----

== 8.3 예제: 전복 나이 예측하기
[source,python]
----
def rssError(yArr,yHatArr): #yArr and yHatArr both need to be arrays
    return ((yArr-yHatArr)**2).sum()

abX, abY = loadDataSet('abalone.txt')
yHat01 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 0.1)
yHat1 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 1)
yHat10 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 10)

print rssError(abY[0:99], yHat01.T)
print rssError(abY[0:99], yHat1.T)
print rssError(abY[0:99], yHat10.T)

yHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)
yHat1 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)
yHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)

print rssError(abY[0:99], yHat01.T)
print rssError(abY[0:99], yHat1.T)
print rssError(abY[0:99], yHat10.T)
----

* 낮은 커널값 이용시 과적합 발생 가능!

== 8.4 데이터를 이해하기 위한 축소 계수
* 선형회귀는 데이터의 속성보다 데이터 점이 적은 경우 역행렬을 구하지 못해 예측 불가
* 이를 해결하기 위해 능형 회귀(ridge regression) 도입
* 이번장은 능형회귀와 라소(lasso), 전방향 단계적 회귀(forward stagewise regression) 을 살펴볼것이다.

=== 8.4.1 능형 회귀
* 능형 회귀는 행렬 X^T^X 에 추가행렬 λI를 더해 비특이 행렬로 만든다.
* λ는 사용자 정의 스칼라값
* w' = ( X^T^ X + λI )^-1^ X^T^y
* 원래 점보다 속성이 많은 경우 문제를 풀기 위해 도입됐지만, 편향(bias)를 추가해 좋은 추정치를 얻는데도 도움이 된다.
* λ는 모든 ws의 합계에 최대값을 부여하기 위해 사용되며, 이렇게 패널티를 부여해 주요하지 않은 매개변수를 줄인다. > 축소 라 부름
* 예측 오류가 최소가 되는 λ를 선택한다.

[source,python]
----
def ridgeRegres(xMat,yMat,lam=0.2):
    xTx = xMat.T*xMat
    denom = xTx + eye(shape(xMat)[1])*lam
    if linalg.det(denom) == 0.0:
        print "This matrix is singular, cannot do inverse"
        return
    ws = denom.I * (xMat.T*yMat)
    return ws

def ridgeTest(xArr,yArr):
    xMat = mat(xArr); yMat=mat(yArr).T
    yMean = mean(yMat,0)
    yMat = yMat - yMean     #to eliminate X0 take mean off of Y
    #regularize X's
    xMeans = mean(xMat,0)   #calc mean then subtract it off
    xVar = var(xMat,0)      #calc variance of Xi then divide by it
    xMat = (xMat - xMeans)/xVar
    numTestPts = 30
    wMat = zeros((numTestPts,shape(xMat)[1]))
    for i in range(numTestPts):
        ws = ridgeRegres(xMat,yMat,exp(i-10))
        print "ws : ", ws
        wMat[i,:]=ws.T
    return wMat

abX, abY = loadDataSet('abalone.txt')
ridgeWeights = ridgeTest(abX, abY)

import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(ridgeWeights)
plt.show()
----

image::imgs/8_4_1.png[,400]

* 아주 작은 log(λ) 는 선현회귀와 같고, 높아질수록 모든 항목의 가중치가 작아지는 것을 볼 수 있다.
* 중간의 적절한 결과를 이용하면 속성 축소로 효과를 볼 수 있다.

=== 8.4.2 라소
* 일반적으로 MSE 회귀를 이용하면 속성간 상관관계가 있을때 매우 큰 양/음의 가중치를 갖게 된다. 이를 피하기 위해 능형회귀에서는 다음과 같은 제약을 사용한다.
** image:imgs/8_4_2.png[,100]
* 라소(lasso)는 다음의 가중치를 이용한다. image:imgs/8_4_3.png[,100]
** λ 값이 충분히 작으면 일부 가중치가 0이돼 데이터를 이해하기 쉽다.
** 문제를 해결하기 위해서는 복잡한 알고리즘이 필요. 전방향 단계별 회귀를 이용하면 거의 비슷한 결과를 도출 가능

== 8.4.3 전방향 단계별 회귀 (forward stagewise regression)
* 처음에는 모든가중치를 0으로, 이후 단계별로 작은 계수로 가중치를 높이거나 낮추면서 만듬

[source,python]
----
def regularize(xMat):#regularize by columns
    inMat = xMat.copy()
    inMeans = mean(inMat,0)   #calc mean then subtract it off
    inVar = var(inMat,0)      #calc variance of Xi then divide by it
    inMat = (inMat - inMeans)/inVar
    return inMat

def stageWise(xArr,yArr,eps=0.01,numIt=100):
    xMat = mat(xArr); yMat=mat(yArr).T
    yMean = mean(yMat,0)
    yMat = yMat - yMean     #can also regularize ys but will get smaller coef
    xMat = regularize(xMat)
    m,n=shape(xMat)
    returnMat = zeros((numIt,n)) #testing code remove
    ws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy()
    for i in range(numIt):
        print i, ws.T
        lowestError = inf;
        for j in range(n):
            for sign in [-1,1]:
                wsTest = ws.copy()
                wsTest[j] += eps*sign
                yTest = xMat*wsTest
                rssE = rssError(yMat.A,yTest.A)
                if rssE < lowestError:
                    lowestError = rssE
                    wsMax = wsTest
        ws = wsMax.copy()
        returnMat[i,:]=ws.T
    return returnMat


abX, abY = loadDataSet('abalone.txt')
res = stageWise(abX, abY, 0.01, 200)
print res
----

* 반복 단계를 늘리면 결과가 일반 선형회귀와 동일해진다.
* 이 알고리즘으로 중요한 속성을 찾고, w 값들을 갖는 다양한 모델을 구축. 10교차검증 등을 통해 가장 좋은 모델은 선택한다.

== 8.5 편향(bias) / 분산(variance) 관계

[#img-sunset]
.traning error vs test error plot
image::imgs/8_5_1.png[,400]

* 일반벅으로 오류는 편향(bias), 오류, 임의노이즈 의 세가지 구성요소의 합
* 8.2, 8.3절에서 커널을 통해 모델에 분산(variance)를 추가
* 반대로 8.4절에서는 축소를 통해 편향(bias)를 추가
