= 9. 트리 기반 회귀
* 하나의 선형 모델로만은 표현이 불가능한 경우 트리와 회귀를 결합해 모델을 구축하는 방법이 있다.
* 트리 구축을 위해 CART(Classification And Regression Trees) 알고리즘을 이용한다.
* 과적합 방지를 위해 가치지기(prunin) 을 이용한다.
* 모델 트리는 회귀트리에서처럼 평균값을 이용하는것이 아닌, 각각 단말 노드에 선형 모델을 구축한다.
* Tkinter 모듈을 이용해 파이썬에서 GUI 생성하는 방법을 살펴본다.

image::imgs/9_4_3.png[,800]

image::imgs/9_4_4.png[,800]

== 9.1 지역적으로 복잡한 데이터 모델링하기
* 트리 기반 회귀의 장.단점
** 장점 : 비선형처럼 복잡한 데이터에 적합
** 단점 : 결과를 해석하기 어려움
** 활용 : 수치형, 명목형
* 3장 의사걸졍에서 사용됐던 알고리즘은 ID3
** 정보이득이 가장 큰 속성 선택, 분할
** 이미 분할된 속성은 더이상 분할의 조건이 되지 않는다.
** 이산형 값만 분할 가능 (연속적인 값은 이산형 값으로 변경 필요)
* CART
** 이진 분할과 연속적인 변화를 제어하는 알고리즘
** CART 알고리즘에 간단한 수정으로 회귀를 다룰 수 있음
** 섀넌 엔트로피 대신 다른 방법으로 복잡도 측정 필요

== 9.2 연속적이고 이산적인 속성으로 트리 구축하기
* CART는 이진분할만 사용, 아래와 같은 노드 구조를 갖는다.

[source,python]
----
class treeNode():
    def __inif__(self, feat, val, right, left):
        fetureToSplitOn = feat
        valueOfSplit = val
        rightBranch = right
        leftBranch = left
----

* 회귀 트리 : 단말노드의 값이 단일값
* 모델 트리 : 단말노드의 갑이 방정식

[source,python]
----
def loadDataSet(fileName):      #general function to parse tab -delimited floats
    dataMat = []                #assume last column is target value
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = map(float,curLine) #map all elements to float()
        dataMat.append(fltLine)
    return dataMat

def binSplitDataSet(dataSet, feature, value):
    # nonzero : Returns a tuple of arrays, containing the indices of the non-zero elements in that dimension.
    m0 = dataSet[nonzero(dataSet[:,feature] > value)[0],:]
    m1 = dataSet[nonzero(dataSet[:,feature] <= value)[0],:]
    return m0, m1

def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):#assume dataSet is NumPy Mat so we can array filtering
    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)#choose the best split
    if feat == None: return val #if the splitting hit a stop condition return val
    retTree = {}
    retTree['spInd'] = feat
    retTree['spVal'] = val
    lSet, rSet = binSplitDataSet(dataSet, feat, val)
    retTree['left'] = createTree(lSet, leafType, errType, ops)
    retTree['right'] = createTree(rSet, leafType, errType, ops)
    return retTree
----

== 9.3 회귀를 위해 CART 사용하기
* 회귀 트리 방법은 단말 노드에 상수 값을 갖는 트리를 사용해 데이터 분할
** 연속적인 값의 무질서도(disorder)를 측정하는 방법 : SSE(Error Sum of Square)

=== 9.3.1 트리 구축하기

[source,python]
----
def regLeaf(dataSet): #returns the value used for each leaf
    return mean(dataSet[:,-1])

def regErr(dataSet):
    return var(dataSet[:,-1]) * shape(dataSet)[0] # return sum of deviation (편차들의 합)

def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):
    tolS = ops[0] # 분할을 하기 위한 오차 임계치
    tolN = ops[1] # 분할을 하기 위한 브랜치의 엘리먼트 수 임계치

    #if all the target variables are the same value: quit and return value
    if len(set(dataSet[:,-1].T.tolist()[0])) == 1: #exit cond 1
        return None, leafType(dataSet)

    m, n = shape(dataSet)

    #the choice of the best feature is driven by Reduction in RSS error from mean
    S = errType(dataSet)
    bestS = inf; bestIndex = 0; bestValue = 0
    for featIndex in range(n-1):
        for splitVal in set(dataSet[:,featIndex].flat):
            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)

            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):
                continue # 균등하게 분할되지 않으면 패스

            newS = errType(mat0) + errType(mat1)
            if newS < bestS:
                bestIndex = featIndex
                bestValue = splitVal
                bestS = newS

    #if the decrease (S-bestS) is less than a threshold don't do the split
    if (S - bestS) < tolS:
        return None, leafType(dataSet) #exit cond 2

    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)
    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):  #exit cond 3
        return None, leafType(dataSet)

    # returns the best feature to split on and the value used for that split
    return bestIndex, bestValue
----

=== 9.3.2 코드 실행하기

[source,python]
----
myMat = mat(loadDataSet('ex0.txt'))
tree = createTree(myMat)
print tree

# {'spInd': 1, 'spVal': '0.39435', 'right': {'spInd': 1, 'spVal': '0.197834', 'right': '-0.023838155555555553', 'left': '1.0289583666666666'}, 'left': {'spInd': 1, 'spVal': '0.582002', 'right': '1.980035071428571', 'left': {'spInd': 1, 'spVal': '0.797583', 'right': '2.9836209534883724', 'left': '3.9871632'}}}
----

== 9.4 트리 가지치기
* 많은 노드를 갖는 트리의 경우 과적합 발생 가능 > 교차검증으로 확인 가능
* 가치지기(pruning)
** 사전 가지치기 : 분할시 특정 갯수 이상의 사건이 포항돼야 분할
** 사후 가지치기 : ?

=== 9.4.1 사전 가지치기
* 이전에 구축한 트리는 tolS와 tolN에 (사전 가지치기 임계치) 민감하다.
* 특히 tolS는 오차값의 구체적인 수치를 의미해 데이터셋의 분산에 따라 민감하게 반응한다.

.그림 9.1
image::imgs/9_4_1.png[,400]

.그림 9.2 : 그림 9.1 의 데이터 공간을 100배로 늘림
image::imgs/9_4_2.png[,400]

* 그림 9.1은 두개의 단말노드로 분할되지만 그림 9.2는 네개의 단말노드로 분할됨

=== 9.4.2 사후 가지치기
* 사후 가지치기 방법
** 트레이닝 셋을 통해 단말 노드가 하나가 될때까지 분할을 반복한다.
** 테스트 셋을 이용해 단말 노드의 병합 전 / 후 오차를 비교한다. 오차가 줄어들 경우 단말 노드를 병합한다.

[source,python]
----
def isTree(obj):
    return (type(obj).__name__=='dict')

def getMean(tree):
    if isTree(tree['right']): tree['right'] = getMean(tree['right'])
    if isTree(tree['left']): tree['left'] = getMean(tree['left'])
    return (tree['left']+tree['right'])/2.0

def prune(prevTree, testData):
    tree = prevTree.copy()
    if shape(testData)[0] == 0: return getMean(tree) #if we have no test data collapse the tree
    if (isTree(tree['right']) or isTree(tree['left'])):#if the branches are not trees try to prune them
        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])
    if isTree(tree['left']): tree['left'] = prune(tree['left'], lSet)
    if isTree(tree['right']): tree['right'] =  prune(tree['right'], rSet)
    #if they are now both leafs, see if we can merge them
    if not isTree(tree['left']) and not isTree(tree['right']):
        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])
        errorNoMerge = sum(power(lSet[:,-1] - tree['left'],2)) +\
            sum(power(rSet[:,-1] - tree['right'],2))
        treeMean = (tree['left']+tree['right'])/2.0
        errorMerge = sum(power(testData[:,-1] - treeMean,2))
        if errorMerge < errorNoMerge:
            # print "merging"
            return treeMean
        else: return tree
    else: return tree


def getDepth(tree):
    left = 1
    right = 1
    if isTree(tree['left']):
        left = getDepth(tree['left']) + 1
    if isTree(tree['right']):
        right = getDepth(tree['right']) + 1
    return max(left, right)

def getTotalNode(tree):
    left = 1
    right = 1
    if isTree(tree['left']):
        left = getTotalNode(tree['left']) + 1
    if isTree(tree['right']):
        right = getTotalNode(tree['right']) + 1
    return left + right

myMat = mat(loadDataSet('ex2.txt'))
tree = createTree(myMat, ops=(0, 1))
myMatTest = mat(loadDataSet('ex2test.txt'))
tree2 = prune(tree, myMatTest)

print getDepth(tree)
print getTotalNode(tree)
print getDepth(tree2)
print getTotalNode(tree2)
----

* 가지치기 전 뎁스는 25, 총 노드수는 392 였으며 가치지기 후 뎁스는 23, 총 노드수 280이다.
* 사전 가지치기만큼 훌륭하지 않다. > 사전 가치지기가 더욱 중요하다.

== 9.5 모델 트리
* 단말노드가 상수값을 갖지 않고 구간별 선형 모델을 갖음

.구간별 선형 (Peicewise linear)
image::imgs/9_5_1.png[,400]

* 구간별 선형의 장점
** 적은수의 선형 모델로 이루어진 작은 트리는 다수의 상수로 이루어진 큰 트리보다 해석하기 쉽다
** 정확도가 높다

[source,python]
----
def linearSolve(dataSet):   #helper function used in two places
    m, n = shape(dataSet)
    X = mat(ones((m, n)))
    # Y = mat(ones((m, 1)))

    #create a copy of data with 1 in 0th postion and strip out Y
    X[:, 1:n] = dataSet[:, 0:n-1]
    Y = dataSet[:, -1]
    xTx = X.T*X
    if linalg.det(xTx) == 0.0:
        raise NameError('This matrix is singular, cannot do inverse,\n\
        try increasing the second value of ops')
    ws = xTx.I * (X.T * Y)
    return ws, X, Y

def modelLeaf(dataSet): #create linear model and return coeficients
    ws, X, Y = linearSolve(dataSet)
    return ws

def modelErr(dataSet):
    ws, X, Y = linearSolve(dataSet)
    yHat = X * ws
    return sum(power(Y - yHat, 2))

# {'spInd': 0, 'spVal': '0.285477', 'right': matrix([[3.46877936],[1.18521743]]),
#     'left': matrix([[1.69855694e-03],[1.19647739e+01]])}
----

== 9.6 예제: 일반 회귀와 트리 방법 비교
# TODO
