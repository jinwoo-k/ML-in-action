= 5장. 로지스틱 회귀
* 회귀는 가장 알맞은 매개변수 집합을 찾으려는 양상을 보임
* 최적의 매개변술르 찾기 위해 최적화 알고리즘 사용

== 5.1 로지스틱 회귀와 시그모이드 함수로 분류하기 (다루기 쉬운 단계)
* 로지스틱 회귀 장단점
** 장점 : 계산 비용이 적고, 구현하기 쉬우며, 결과 해석을 위한 지식 표현이 쉽다.
** 단점 : 언더피팅 경향이 있어, 정확도가 낮게 나올 수 있다.
** 활용 : 수치형 값, 명목형 값
* 간혹 계단 함수 같은 경우 분할(분류)를 어렵게 한다. (0 또는 1로만 표현돼 강도를 알 수 없음)
* 시그모이드 함수 ( image:imgs/5_1_3.png[step,100] ) 를 이용해 계단함수의 단점을 보완

.step function
image::imgs/5_1_1.png[step,300,200]

.sigmoid function
image::imgs/5_1_2.png[step,300,200]

* 로지스틱 회귀 분류기는 각각의 속성에 가중치를 곱한 후 모두 더한다. 그 결과를 시그모이드에 넣어 0에서 1 사이의 수를 구한다. 이 수가 0.5 이상이면 1로, 이하면 0으로 분류한다.

== 5.2 가장 좋은 회귀 계수를 찾기 위해 최적화 사용하기
* 시그모이드 함수의 입력 z 는 다음과 같다.
** z = w~0~x~0~ + w~1~x~1~ + w~2~x~2~ + ... + w~n~x~n~
** z = w^T^x (벡터 형식으로 표기)

=== 5.2.1 기울기 상승
* 함수에서 최대 지점을 찾고자 할때 이용
* 함수 f(x, y)의 기울기 함수 : image:imgs/5_2_1.png[50,150]
* 기울기 상승 함수 : image:imgs/5_2_2.png[50,150]
** 멈춤 조건에 도달할때까지 되풀이됨 (반복횟수초과 또는 오차범위안에 결과 도달)

.기울기 상승 과정
image::imgs/5_2_3.png[50,150]

=== 5.2.2 훈련 : 기울기 상승을 사용하여 가장 좋은 매개변수 찾기

[source,python]
----
from numpy import *

def loadDataSet():
    dataMat = []; labelMat = []
    fr = open('testSet.txt')
    for line in fr.readlines():
        lineArr = line.strip().split()
        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])
        labelMat.append(int(lineArr[2]))
    return dataMat,labelMat

def sigmoid(inX):
    return 1.0/(1+exp(-inX))

def gradAscent(dataMatIn, classLabels):
    dataMatrix = mat(dataMatIn)             #convert to NumPy matrix
    labelMat = mat(classLabels).transpose() #convert to NumPy matrix
    m,n = shape(dataMatrix) # 100*3
    alpha = 0.001
    maxCycles = 100
    weights = ones((n,1))   # 3*1
    for k in range(maxCycles):              #heavy on matrix operations
        h = sigmoid(dataMatrix * weights)   #matrix mult 100*1
        error = (labelMat - h)              #vector subtraction 100*1
        print "error", error.transpose()
        weights = weights + alpha * dataMatrix.transpose()* error #matrix mult 3*1 = 3*1 + n*3*100*100*1
        # print "weight ", weights.transpose()
    return weights

dataMat, labelMat = loadDataSet()
gradAscent(dataMat, labelMat)
----

=== 5.2.3 분석 : 의사결정 경계선 플롯하기

[source,python]
----
def plotBestFit(weights):
    import matplotlib.pyplot as plt
    dataMat,labelMat=loadDataSet()
    dataArr = array(dataMat)
    n = shape(dataArr)[0]
    xcord1 = []; ycord1 = []
    xcord2 = []; ycord2 = []
    for i in range(n):
        if int(labelMat[i])== 1:
            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])
        else:
            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')
    ax.scatter(xcord2, ycord2, s=30, c='green')
    x = arange(-3.0, 3.0, 0.1)
    # 0 = w0x0 + w1x1 + w2x2 > x2 = (- w0x0 - w2x2) / w2
    y = (-weights[0] - weights[1] * x) / weights[2]
    ax.plot(x, y)
    plt.xlabel('X1'); plt.ylabel('X2')
    plt.show()
----

image::imgs/5_2_4.png[150,400]

=== 5.2.4 훈련 : 확률적인 기울기 상승
* 확률 기울기 상승
** 앞의 예에서는 데이터셋이 100개로 구성되어 연산이 가능했지만, 데이터의 크기가 커지면 모든 데이터에 대해 계산이 힘들다.
** 한번에 하나의 사례를 이용해 가중치 갱신
** 온라인 학습 알고리즘이라 불림 (vs 일괄 처리)

[source,python]
----
def stocGradAscent0(dataMatrix, classLabels):
    m,n = shape(dataMatrix)
    alpha = 0.01
    weights = ones(n)   #initialize to all ones
    for i in range(m):
        h = sigmoid(sum(dataMatrix[i]*weights))
        error = classLabels[i] - h
        weights = weights + alpha * error * dataMatrix[i]
    return weights
----

image::imgs/5_2_5.png[150,400]

* 기존의 결과 (500회 반복) 보다 품질이 좋지 않음. > 반복 횟수 수정

.반복에 따른 weight 변화
image::imgs/5_2_6.png[150,400]

[source,python]
----
def stocGradAscent1(dataMatrix, classLabels, numIter = 150):
    m,n = shape(dataMatrix)
    alpha = 0.01
    weights = ones(n)   #initialize to all ones
    for j in range(numIter):
      dataIndex = range(m)
      for i in range(m):
          alpha = 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not
          randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant
          h = sigmoid(sum(dataMatrix[randIndex]*weights))
          error = classLabels[randIndex] - h
          weights = weights + alpha * error * dataMatrix[randIndex]
          del(dataIndex[randIndex])
  return weights
----

* 반복이 진행됨에 따라 alpha 값을 감소시켜 진동을 줄임
* 하나의 반복 내에서 선택되는 레코드의 순서를 무작위로 변경

.반복에 따른 weight 변화
image::imgs/5_2_7.png[150,400]

* 각 반복 안에서 항목 랜덤 추출로 인해 규칙성 제거

== 5.3 예제 : 말의 배앓이 치사율 평가하기
* 속성이 28개인 368개 사례로 부터 말의 생사를 판단하기 위한 계수 생성

=== 5.3.1 준비 : 데이터에서 누락된 값 다루기
* 누락된 값을 다루는 방법
** 평균값 이용
** -1과 같은 특별한 값으로 채움
** 해당 사례를 무시
** 유사한 아이템들로부터 평균값 사용
** 다른 기계학습 알고리즘 이용

* 이번에서는 누락된 값을 0으로 채우도록 함
** sigmoid(0) = 0.5 로 해당 값이 결과에 영향을 미치지 않음

* 분류 항목(타겟)이 누락된 데이터는 무시(제외)

=== 5.3.2 검사 : 로지스틱 회귀로 분류하기

[source, python]
----
def classifyVector(inX, weights):
    prob = sigmoid(sum(inX*weights))
    if prob > 0.5: return 1.0
    else: return 0.0

def colicTest():
    frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt')
    trainingSet = []; trainingLabels = []
    for line in frTrain.readlines():
        currLine = line.strip().split('\t')
        lineArr =[]
        for i in range(21):
            lineArr.append(float(currLine[i]))
        trainingSet.append(lineArr)
        trainingLabels.append(float(currLine[21]))
    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000)
    errorCount = 0; numTestVec = 0.0
    for line in frTest.readlines():
        numTestVec += 1.0
        currLine = line.strip().split('\t')
        lineArr =[]
        for i in range(21):
            lineArr.append(float(currLine[i]))
        if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]):
            errorCount += 1
    errorRate = (float(errorCount)/numTestVec)
    print "the error rate of this test is: %f" % errorRate
    return errorRate
----
