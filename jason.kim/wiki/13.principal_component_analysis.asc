= 13장 데이터 간소화를 위한 주요 구성요소 분석 사용하기
* Using principal component analysis to simplify data
* 이번장은 차원축소와 주요 구성요소 분석에 대해 살펴본다.

== 13.1 차원 축소 기술
* 데이터를 단순화 하려는 이유
** 사용하기 쉬운 데이터 집합 구축
** 많은 알고리즘의 계산 비용 축소
** 노이즈 제거
** 이해하기 쉬운 결과 도출
* 이번장에서 다룰 차원축소 기술은 분류항목 표시가 있을때와 없을때 모두 사용 가능
** 주요 구성요소 분석(principal component analysis, PCA)
*** 변화량이 가장 큰 데이터가 한개 축을, 그와 직교하는 다른 데이터를 다른 하나의 축으로 선택
** 요소 분석 (factor analysis)
*** 잠재적인 변수(latent variable)을 관찰 가능한 데이터로 생성, 잠재적 변수는 관찰가능한 변수보다 적기 때문에 차원축소 기능 수행
** 독립적인 구성요소 분석(independent component analisys, ICA)
*** 요소 분석과 비슷하게 데이터가 N개의 자료로 생성된다고 가정, PCA 와 달리 정적이며 독립적(상관관계X)이라고 간주
* 위의 세가지 차원축소 기술 중 PCA가 가장 널리 쓰이며, 이번장은 PCA에 초점을 둠

== 13.2 주요 구성요소 분석 (PCA)

=== 13.2.1 좌표 축 이동
* 첫번째 주요 구성요소 선택 : 변화량이 가장 큰 데이터의 방향
* 두번째 주요 구성요소 선택 : 변화량이 두번째로 크며, 첫번째 주요 구성요소와 직교하는 방향의 데이터 선택
* 공분산 행렬로부터 고유값 분석을 통해 N개의 주요 구성요소 선택

=== 13.2.2 NumPy에서 PCA 수행하기

.상위 N개의 주요 구성요소로 데이터를 변형하는 의사코드
----
Remove the mean
// 각각의 데이터에서 평균을 차감
Compute the covariance matrix
// 공분산 행렬 계산
Find the eigenvalues and eigenvectors of the covariance matrix
// 공분산 행렬의 고유값과 고유벡터 찾기
Sort the eigenvalues from largest to smallest
// 고유값을 이용해 내림차순으로 정렬
Take the top N eigenvectors
// 상위 N개의 고유벡터 선택
Transform the data into the new space created by the top N eigenvectors
// 데이터를 상위 N개의 고유백터를 통해 새로운 공간의 데이터로 변환
----

[source,python]
----
from numpy import *

def loadDataSet(fileName, delim='\t'):
    fr = open(fileName)
    strs_list = [line.strip().split(delim) for line in fr.readlines()]
    float_list_list = [map(float, strs) for strs in strs_list]
    return mat(float_list_list)

def pca(dataMat, topNfeat=9999999):
    meanVals = mean(dataMat, axis=0)
    meanRemoved = dataMat - meanVals    # remove mean
    covMat = cov(meanRemoved, rowvar=0)
    eigVals, eigVects = linalg.eig(mat(covMat))
    eigValInd = argsort(eigVals)              # sort, sort goes smallest to largest
    eigValInd = eigValInd[:-(topNfeat+1):-1]  # cut off unwanted dimensions
    redEigVects = eigVects[:, eigValInd]      # reorganize eig vects largest to smallest
    lowDDataMat = meanRemoved * redEigVects   # transform data into new dimensions
    reconMat = (lowDDataMat * redEigVects.T) + meanVals
    return lowDDataMat, reconMat

dataMat = loadDataSet('testSet.txt')
lowDMat, reconMat = pca(dataMat, 2)

import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(dataMat[:,0].flatten().A[0], dataMat[:,1].flatten().A[0], marker='^', s=90)
ax.scatter(reconMat[:,0].flatten().A[0], reconMat[:,1].flatten().A[0], marker='o', s=50, c='red')
plt.show()
----

== 13.3 예제: PCA로 반도체 제조 데이터 차원 축소하기
* 590개 속성이 있는 자료에 대해 차원축소를 진행하며, 많은 속성의 경우 NaN값이 존재한다. > 평균값으로 치환

[source,python]
----
def replaceNanWithMean():
    datMat = loadDataSet('secom.data', ' ')
    numFeat = shape(datMat)[1]
    # replaced = []
    for i in range(numFeat):
        meanVal = mean(datMat[nonzero(~isnan(datMat[:, i].A))[0], i]) #values that are not NaN (a number)
        # replaced.append(str(shape(nonzero(isnan(datMat[:, i].A))[0])[0]))
        datMat[nonzero(isnan(datMat[:, i].A))[0], i] = meanVal  #set NaN values to mean
    # print ','.join(replaced)
    return datMat

dataMat = replaceNanWithMean()
meanVals = mean(dataMat, axis=0)
meanRemoved = dataMat - meanVals
covMat = cov(meanRemoved, rowvar=0)
eigVals, eigVects = linalg.eig(mat(covMat))
----

* 위의 결과에서 고유값(eigVals)들 중 20% 이상이 0이며, 이는 다른 속성을 복사한것에 지나지 않는다는 의미이다.
* TODO 책 351에 그림 13.4(전체 변화량에 대한 백분률) 은 어떻게 구하는것인지 확인하기
