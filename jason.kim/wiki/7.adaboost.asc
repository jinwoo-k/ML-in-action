= 7장. 에이다부스트 메타 알고리즘으로 분류 개선하기
* 메타 알고리즘 : 서로 다른 알고리즘을 결합해 더 나은 결과를 얻는 방식
* 에이다부스트 : 가장 인기 좋은 메타 알고리즘
* 이번 장에서 다루는 내용들
** 앙상블 메소드에 대해 논의 및 부스팅 알고리즘인 에이다부스트 학습
** 에이다부스트 적용을 위한 단일 노드 의사결정트리인 의사결정 스텀프 분류기 구축
** 분류기가 갖는 일반적인 문제인 불균형(classification imbalance)에 대한 논의

== 7.1 데이터 집합의 다양한 표본을 사용하는 분류기
* 에이다부스트의 장단점
** 장점 : 오류율이 낮고 코드가 쉽다. 가장 좋은 분류기를 가지고 본류하며, 조절을 위한 매개변수가 없다.
** 단점 : 오류 데이터(outlier)에 민감하다.
** 활용 : 수치형 값, 명목형 값

* 다양한 분류기를 결합하는 방법을 앙상블 메소드 혹은 메타 알고리즘이라 부른다.

=== 7.1.1 배깅 : 임의로 추출한 재표본 데이터로부터 분류기 구축하기
* 배깅이란? +
배깅(영어: bagging)은 bootstrap aggregating의 줄임말로 통계적 분류와 회귀 분석에서 사용되는 기계 학습 알고리즘의 안정성과 정확도를 향상시키기 위해 고안된 일종의 앙상블 학습법의 메타 알고리즘이다. 또한 배깅은 분산을 줄이고 과적합(overfitting)을 피하도록 해준다. 결정 트리 학습법이나 랜덤 포레스트에만 적용되는 것이 일반적이기는 하나, 그 외의 다른 방법들과 함께 사용할 수 있다.

* 설명 +
n크기의 훈련 집합(training set) D가 주어졌을 때, 배깅은 m개의 복원 표본추출(sampling with replacement) 방법과 균등 확률분포를 이용해 각각 n′크기를 갖는 새로운 훈련 집합 D~i~을 생성한다. +
복원 표본추출 방법에 의해 일부 관측 데이터는 각 D~i~에서 반복해서 나타날 수 있다. +
만약 n′=n이라고 하면, 보다 큰 n에 대해 집합 D_~i~는 D에 대해 고유한 샘플의 비율은 (1 - 1/e) (≈63.2%)을 가질 것으로 기대된다. +
이러한 샘플을 부트스트랩(bootstrap) 샘플이라 한다. +
m개의 모델은 m개의 부트스트랩 샘플들을 이용해 만들어지고 결과를 평균(회귀분석의 경우) 또는 투표(분류의 경우)를 통해 결합한다.

=== 7.1.2 부스팅
* 배깅과 유사함, 배깅은 동일한 유형의 분류기를 사용하는데 비해 부스팅은 다른 유형의 분류기 사용
* 가중치를 부여한 모든 분류기의 합계로 계산

=== 7.2 훈련 : 오류에 초점을 맞춘 분류기 개선
* 에이다부스트(AdaBoost)는 적용형 부스팅(adaptive boosting)의 줄임말
* TODO 알고리즘 설명

=== 7.3 의사결정 스텀프로 약한 학습기 생성하기
* 의사결정 스텀프(stump) : 간단한 의사결정 트리로 단 하나의 속성에 대해 하나의 의사결정을 만든다.

[source,python]
----
# 알고리즘을 시험해볼 수 있는 간단한 데이터 집합, 45 문제라 부르며 의사결정 트리로 해결하지 못하는 문제이다.
def loadSimpData():
    datMat = matrix([[1., 2.1],
                     [2., 1.1],
                     [1.3, 1.],
                     [1., 1.],
                     [2., 1.]])
    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]
    return datMat, classLabels

# 특정 속성(dimension)에 대해 임계치를 이용해 분류 (각 레코드를 1 또는 -1로 세팅)
def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):  # just classify the data
    retArray = ones((shape(dataMatrix)[0], 1))
    if threshIneq == 'lt':
        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0
    else:
        retArray[dataMatrix[:, dimen] > threshVal] = -1.0
    return retArray

# 데이터 집합으로부터  가중치 D를 이용해 가장 분류가 잘 되는 stump 를 찾아 반환하는 메서드
def buildStump(dataArr, classLabels, D):
    dataMatrix = mat(dataArr)
    labelMat = mat(classLabels).T
    m, n = shape(dataMatrix)
    numSteps = 10.0
    bestStump = {}
    bestClasEst = mat(zeros((m, 1)))
    minError = inf  # init error sum, to +infinity
    for i in range(n):  # loop over all dimensions
        rangeMin = dataMatrix[:, i].min()
        rangeMax = dataMatrix[:, i].max()
        stepSize = (rangeMax - rangeMin) / numSteps
        for j in range(-1, int(numSteps) + 1):  # loop over all range in current dimension
            for inequal in ['lt', 'gt']:  # go over less than and greater than
                threshVal = (rangeMin + float(j) * stepSize)
                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)  # call stump classify with i, j, lessThan
                errArr = mat(ones((m, 1)))
                errArr[predictedVals == labelMat] = 0
                weightedError = D.T * errArr  # calc total error multiplied by D
                # print "split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f" % (i, threshVal, inequal, weightedError)
                if weightedError < minError:
                    minError = weightedError
                    bestClasEst = predictedVals.copy()
                    bestStump['dim'] = i
                    bestStump['thresh'] = threshVal
                    bestStump['ineq'] = inequal
    return bestStump, minError, bestClasEst

# 테스트
dataMat, classLabels = loadSimpData()
D = mat(ones((5, 1))/5)
print buildStump(dataMat, classLabels, D)
----

* 위의 buildStump 눈여겨 볼 부분 : numSteps 를 +1부터 0까지 진행 > 양쪽 범위 바깥쪽 임계값을 설정함

== 7.4 전체 에이다부스트 알고리즘 구현하기

* 이전 절에서 만들었던 의사결정 스텀프를 이용하는 에이다부스트 알고리즘을 구현한다.

[source,python]
----
def adaBoostTrainDS(dataArr, classLabels, numIt=40):
    weakClassArr = []
    m = shape(dataArr)[0]
    D = mat(ones((m, 1)) / m)  # init D to all equal
    aggClassEst = mat(zeros((m, 1)))
    for i in range(numIt):
        print
        print "D for current step : ", D.T

        bestStump, error, classEst = buildStump(dataArr, classLabels, D)  # build Stump

        print "error: ", error

        alpha = float(
            0.5 * log((1.0 - error) / max(error, 1e-16)))  # calc alpha, throw in max(error,eps) to account for error=0
        # if error is 0.2 then alpha is 0.6xxx
        # if error is 0.5 then alpha is 0
        # if error is 0.8 then alpha is -0.69xxx

        print "alpha: ", alpha

        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)  # store Stump Params in Array

        print "classEst: ",classEst.T

        expon = multiply(-1 * alpha * mat(classLabels).T, classEst)  # exponent for D calc, getting messy

        print "expon: ", expon.T
        print "exp(expon): ", exp(expon).T

        D = multiply(D, exp(expon))  # Calc New D for next iteration
        D = D / D.sum()

        print "D for next step: ", D.T

        # calc training error of all classifiers, if this is 0 quit for loop early (use break)
        aggClassEst += alpha * classEst

        print "aggClassEst: ", aggClassEst.T
        print "aggClassEst: ", sign(aggClassEst).T

        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T, ones((m, 1)))
        errorRate = aggErrors.sum() / m

        print "total error: ", errorRate

        if errorRate == 0.0: break
    return weakClassArr, aggClassEst
----

----
심플 데이터를 이용한 테스트

dataMat, classLabels = loadSimpData()
print classLabels

classifierArray = adaBoostTrainDS(dataMat, classLabels, 9)
print
print classifierArray[0]
print classifierArray[1].T


예측하고자 하는 레이블 : [1.0, 1.0, -1.0, -1.0, 1.0]

error:  [[0.2]]
D:  [[0.2 0.2 0.2 0.2 0.2]]
alpha:  0.69314718056
classEst:  [[-1.  1. -1. -1.  1.]]
expon:  [[ 0.69314718 -0.69314718 -0.69314718 -0.69314718 -0.69314718]]
exp(expon):  [[2.  0.5 0.5 0.5 0.5]]
next D:  [[0.5   0.125 0.125 0.125 0.125]]
aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]
aggClassEst:  [[-1.  1. -1. -1.  1.]]
total error:  0.2

error:  [[0.125]]
D:  [[0.5   0.125 0.125 0.125 0.125]]
alpha:  0.972955074528
classEst:  [[ 1.  1. -1. -1. -1.]]
expon:  [[-0.97295507 -0.97295507 -0.97295507 -0.97295507  0.97295507]]
exp(expon):  [[0.37796447 0.37796447 0.37796447 0.37796447 2.64575131]]
next D:  [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]
aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]
aggClassEst:  [[ 1.  1. -1. -1. -1.]]
total error:  0.2

error:  [[0.14285714]]
D:  [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]
alpha:  0.895879734614
classEst:  [[1. 1. 1. 1. 1.]]
expon:  [[-0.89587973 -0.89587973  0.89587973  0.89587973 -0.89587973]]
exp(expon):  [[0.40824829 0.40824829 2.44948974 2.44948974 0.40824829]]
next D:  [[0.16666667 0.04166667 0.25       0.25       0.29166667]]
aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]
aggClassEst:  [[ 1.  1. -1. -1.  1.]]
total error:  0.0

[{'dim': 0, 'ineq': 'lt', 'thresh': '1.3', 'alpha': 0.6931471805599453}, {'dim': 1, 'ineq': 'lt', 'thresh': '1.0', 'alpha': 0.9729550745276565}, {'dim': 0, 'ineq': 'lt', 'thresh': '0.9', 'alpha': 0.8958797346140273}]
[[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]

----

== 7.5 검사 : 에이다부스트로 분류하기

[source,python]
----
def adaClassify(datToClass, classifierArr):
    dataMatrix = mat(datToClass)  # do stuff similar to last aggClassEst in adaBoostTrainDS
    m = shape(dataMatrix)[0]
    aggClassEst = mat(zeros((m, 1)))
    for i in range(len(classifierArr)):
        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'], \
                                 classifierArr[i]['thresh'], \
                                 classifierArr[i]['ineq'])  # call stump classify
        aggClassEst += classifierArr[i]['alpha'] * classEst
        print aggClassEst
    return sign(aggClassEst)
----

== 7.6 예제 : 에이다부스트에 복잡한 데이터 집합 적용하기

[source,python]
----
def loadDataSet(fileName):  # general function to parse tab -delimited floats
    numFeat = len(open(fileName).readline().split('\t'))  # get number of fields
    dataMat = [];
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat - 1):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat, labelMat
----

* 분류기의 갯수를 늘림에 따라 훈련오류는 점차 줄었지만, 검사오류의 경우 점차 줄다가 어느순간부터 다시 증가한다. > 이를 과적합(overfitting)이라 부른다.

== 7.7 분류 불균형
* 경우에 따라서 요구되는 정확도가 다르다.
** 스팸분류의 경우 햄이 스팸으로 분류되지 않는것이 중요
** 암 진단의 경우 암을 암이 아님으로 분류하지 않는것이 중요. 등

=== 7.7.1 또 다른 성능 측정 방법 : 정확도, 재현율 그리고 ROC
* 분류 오류를 표현하기 위해 혼동 행렬(confusion matrix)를 이용한다.

image::imgs/7_7_1.png[300]

* Precision(정밀도) : TP / (TP + FP) -> 1이라고 예측한 것 중 실제 값이 1인 것
* Recall(sesitivity, 재현율, 민감도) : TP / (TP + FN) -> 실제 값이 1인 것 중 1이라고 예측한 것
* Accuracy(정확도) : (TP + TN) / (TP + TN + FP + FN) -> 전체 경우의 수 중 정확히 예측한 수
* Precision 과 Recall 이 동시에 높이기는 쉽지 않다.

* ROC 커브로 : 높은 TP 와 낮은 FP 를 갖게 하는게 좋다.
** 스팸분류의 경우 햄을 스팸이라 분류하지 않도록 FP 를 낮추는게 중요
** 암 분류의 경우 암을 암 아님으로 분류하지 않도록 FN 을 낮추는게 중요

image::imgs/7_7_3.png[300]



* 서로다른 ROC 곡선을 비교하는 방법으로 AUC 사용, 분류기의 성능에 대한 평균값을 구함.
** 1.0은 완벽한 분류기를 의미하며 임의 추측 AUC는 0.5값이 됨

[source,python]
----
def plotROC(predStrengths, classLabels):
    import matplotlib.pyplot as plt
    cur = (1.0, 1.0)  # cursor
    ySum = 0.0  # variable to calculate AUC
    numPosClas = sum(array(classLabels) == 1.0)
    yStep = 1 / float(numPosClas);
    xStep = 1 / float(len(classLabels) - numPosClas)
    sortedIndicies = predStrengths.argsort()  # get sorted index, it's reverse
    fig = plt.figure()
    fig.clf()
    ax = plt.subplot(111)
    # loop through all the values, drawing a line segment at each point
    for index in sortedIndicies.tolist()[0]:
        if classLabels[index] == 1.0:
            delX = 0;
            delY = yStep;
        else:
            delX = xStep;
            delY = 0;
            ySum += cur[1]
        # draw line from cur to (cur[0]-delX,cur[1]-delY)
        ax.plot([cur[0], cur[0] - delX], [cur[1], cur[1] - delY], c='b')
        cur = (cur[0] - delX, cur[1] - delY)
    ax.plot([0, 1], [0, 1], 'b--')
    plt.xlabel('False positive rate');
    plt.ylabel('True positive rate')
    plt.title('ROC curve for AdaBoost horse colic detection system')
    ax.axis([0, 1, 0, 1])
    plt.show()
    print "the Area Under the Curve is: ", ySum * xStep
----

image::imgs/7_7_4.png[300]

=== 7.7.2 비용 함수를 가진 분류기의 의사결정 다루기
* 비용 민감 학습(cost-sensitive learning) : 고르지 못한 분류비용을 활용해 분류기의 임계값을 조정하는 방법
* 아래와 같이 어떤 결과가 더 중요한지 따라 가중치를 줘, 최소비용을 갖는 분류기를 선택할 수 있다.

image::imgs/7_7_5.png[,400]

* 학습 알고리즘 별 가중치 부여 방법
** 에이다부스트는 가중치 벡터 D를 조절
** 나이브 베이즈에서는 가장 높은 확률 대신 가장 낮은 비용으로 항목 예측
** SVM 은 서로다른 분류 항목에 대해 비용함수에서 매개변수 C를 다르게 설정

=== 7.7.3 분류 불균형이 있는 데이터를 처리하기 위한 데이터 샘플링
* 불균형 데이터를 분류할 수 있는 방법으로 언더샘플링(undersampling) 과 오버샘플링(oversampling)이 있다.
* 드물게 발생하는 긍정적인 분류는 그대로 두고, 많이 발생하는 부정적인 분류를 언더샘플링
** 부정적인 신용거래 50건과 정당한 거래 5000건이 있을때 정당한 거래에 대한 부분을 언더샘플링 진행
** 하이브리드 방식 : 긍적적인거은 오버샘플링, 부정적인것은 언더샘플링 > 과적합 발생 가능성이 있음
